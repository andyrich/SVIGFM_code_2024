{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import flopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs \n",
    "import flopy.utils.binaryfile as bf\n",
    "import contextily as ctx\n",
    "import fiona\n",
    "import seaborn as sns\n",
    "import pyemu\n",
    "from pathlib import Path\n",
    "import helpers\n",
    "from shutil import copytree, ignore_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import conda_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "owhm2 = r\"C:\\GSP\\sv\\model\\SV_mod_V2_owhm2\\master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ws = \"local_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2('forward_run.py', template_ws)\n",
    "import forward_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dts = conda_scripts.utils.get_dates.get_dates()\n",
    "ml = conda_scripts.sv_budget.load_sv_model.get_model(owhm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mg = conda_scripts.arich_functions.get_flopy_model_spatial_reference('son')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = helpers.get_sr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "egpath = Path(\".\").absolute()\n",
    "# while egpath.name != 'examples':\n",
    "#     os.chdir('..')\n",
    "#     egpath = Path(\".\").absolute()\n",
    "\n",
    "model_ws = Path(owhm2).absolute()\n",
    "tmp_path = Path(template_ws).absolute()\n",
    "\n",
    "EXE_DIR = Path(\"..\",\"bin\").absolute()\n",
    "if \"window\" in platform.platform().lower():\n",
    "    EXE_DIR = Path(EXE_DIR,\"win\")\n",
    "elif \"darwin\" in platform.platform().lower() or \"macos\" in platform.platform().lower():\n",
    "    EXE_DIR = Path(EXE_DIR,\"mac\")\n",
    "else:\n",
    "    EXE_DIR = Path(EXE_DIR,\"linux\")\n",
    "    \n",
    "# basename = Path(model_ws).name\n",
    "new_d = Path(tmp_path)\n",
    "# if new_d.exists():\n",
    "#     shutil.rmtree(new_d)\n",
    "# Path(tmp_path).mkdir(exist_ok=True)\n",
    "# creation functionality\n",
    "# shutil.copytree(model_ws, new_d, ignore=ignore_patterns('*.pyc', 'output*','.git*'))\n",
    "\n",
    "os.chdir(tmp_path)\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the existing model and save it in a new dir and make sure it runs\n",
    "import flopy\n",
    "# model_ws = new_d.relative_to(tmp_path)\n",
    "# ml = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=model_ws,verbose=False)\n",
    "ml = conda_scripts.sv_budget.load_sv_model.get_model(workspace=tmp_path)\n",
    "# ml.model_ws = \"temp\"\n",
    "# ml.exe_name = \"mfnwt\"\n",
    "# [shutil.copy2(os.path.join(EXE_DIR,f),os.path.join(ml.model_ws,f)) for f in os.listdir(EXE_DIR)]\n",
    "# ml.write_input()\n",
    "# pyemu.os_utils.run(\"mf-owhm.exe SVIGFM_GSP.nam\", cwd=ml.model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## list file budget components as observations (or forecasts)\n",
    "\n",
    "Here we will use ``flopy`` and ``pyemu`` to load each of the flux and volume budget components from the ``modflow`` list file to use as observations (or forecasts).  These are valuable pieces of information and since observations are free, why not include them?  This helper function writes two instruction files: ``<flx_filename>.ins`` and ``<vol_filename>.ins``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the flux budget output filename that will be written during each forward run\n",
    "# flx_filename=os.path.join(owhm2,'output', \"Budget.txt\")\n",
    "\n",
    "# # the volumne budget output filename that will be written during each forward run\n",
    "# vol_filename = os.path.join(ml.model_ws,\"vol.out\")\n",
    "# df_wb = pyemu.gw_utils.setup_mflist_budget_obs(os.path.join(ml.model_ws,ml.name+\".list\"))\n",
    "# df_wb = pyemu.gw_utils.setup_mflist_budget_obs(flx_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Parameters\n",
    "\n",
    "## pilot points\n",
    "\n",
    "Here we will setup pilot points for several array-based ``modflow`` inputs using ``pyemu``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### setup pilot point locations\n",
    "\n",
    "first specify what pilot point names we want to use for each model layer (counting from 0).  Here we will setup pilot points for ``hk``, ``sy`` and ``rech``.  The ``rech`` pilot points will be used as a single multiplier array for all stress periods to account for potential spatial bias in recharge.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prefix_dict = forward_run.get_prefix_dict_for_pilot_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "This helper function is doing a lot of things: writing templates, pilot point files, and creating a shapefile of pilot points.  The ``every_n_cell`` arg is key: it decides how many cells to skip between pilot point locations - since we passed the ``model``, only active model cells get pilot points (using ``bas6.ibound``).  Like many things with ``flopy``, the ``SpatialReference`` is used to define pilot point ``x`` and ``y `` coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(ml.model_ws,'pp2024')):\n",
    "    print(f\"{os.path.join(ml.model_ws,'pp2024')} already exists\")\n",
    "else:\n",
    "    os.mkdir(os.path.join(ml.model_ws,'pp2024'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.genfromtxt(os.path.join(ml.model_ws, 'model_arrays', 'zonation_3.csv'), delimiter = ' ')\n",
    "\n",
    "zotther = z.copy()\n",
    "zotther[zotther>8] = 0\n",
    "\n",
    "zones = {i:zotther  for i in range(7) }\n",
    "zones[0] = z\n",
    "plt.imshow(zones[0])\n",
    "plt.figure()\n",
    "plt.imshow(zones[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_cells = 7\n",
    "pp_df = pyemu.pp_utils.setup_pilotpoints_grid(ml,\n",
    "                                prefix_dict=prefix_dict,\n",
    "                                              ibound = zones,\n",
    "                                              use_ibound_zones = False,\n",
    "                                every_n_cell=pp_cells,\n",
    "                                pp_dir=os.path.join(ml.model_ws,'pp2024'),\n",
    "                                tpl_dir=os.path.join(ml.model_ws,'pp2024'),\n",
    "                                shapename=os.path.join(ml.model_ws,'pp2024',\"pp.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "The ``dataframe`` return has the same info as the shapefile that was written - useful info, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.index = pp_df.parnme\n",
    "pp_df\n",
    "\n",
    "print(pp_df.pargp.unique())\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.parval1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# assign values from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load(folder, var, lay,name):\n",
    "    arr = np.genfromtxt(os.path.join(folder, f\"PARAM_{var}_G1_L{lay}.txt\"), skip_header=1)\n",
    "\n",
    "    x,y = np.indices(arr.shape, dtype = int)\n",
    "    x = x.reshape((-1,1))\n",
    "    y = y.reshape((-1,1))\n",
    "    arr = arr.reshape((-1,1))\n",
    "    arr = pd.DataFrame(data = np.hstack([x,y,arr]), columns = ['i','j', f\"{name}{lay}\"])\n",
    "    arr = arr.set_index(['i','j'])\n",
    "    return arr\n",
    "\n",
    "\n",
    "ar = [load(os.path.join(owhm2, 'output'), 'HKR', layer, 'hk') for layer in np.arange(1,7)]\n",
    "hk = pd.concat(ar, axis = 1,)\n",
    "ar = [load(os.path.join(owhm2, 'output'), 'VKA', layer, 'vk') for layer in np.arange(1,7)]\n",
    "vk = pd.concat(ar, axis = 1,)\n",
    "ar = [load(os.path.join(owhm2, 'output'), 'SS', layer, 'ss') for layer in np.arange(1,7)]\n",
    "ss = pd.concat(ar, axis = 1,)\n",
    "sy = [load(os.path.join(owhm2, 'output'), 'Sy', layer, 'sy') for layer in np.arange(1,2)][0]\n",
    "\n",
    "aq = pd.concat([hk, vk, ss, sy], axis = 1)\n",
    "aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pp_df.columns\n",
    "\n",
    "pp_ren = pd.merge(pp_df, aq, on = ['i', 'j'], how = 'left')\n",
    "\n",
    "for prop in pp_df.pargp.unique():\n",
    "    if prop in aq.columns:\n",
    "        c = pp_ren.pargp== prop\n",
    "        pp_ren.loc[c, 'parval1'] = pp_ren.loc[c, prop]\n",
    "    else:\n",
    "        print(f\"skipping {prop} bc it's not in loaded arrays\")\n",
    "\n",
    "# set vk to 0.1 for layer 1 and 0.001 for others\n",
    "pp_ren.loc[pp_ren.pargp.str.startswith('vk'), 'parval1'] = 0.1\n",
    "pp_ren.loc[pp_ren.pargp=='vk1', 'parval1'] = 0.1\n",
    "\n",
    "\n",
    "pp_ren = pp_ren.loc[:,cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# set bounds to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(input_string):\n",
    "    return re.sub(r'\\d+', '', input_string)\n",
    "\n",
    "pp_ren.pargp.apply(remove_numbers).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = dict(hk = [0.0001, 500],\n",
    "sy = [0.01, 0.3],\n",
    "vk = [0.001, .1],\n",
    "ss = [1E-7, 1E-4],\n",
    "fmp_vk = [0.0001, 10],\n",
    "drn_k= [0.0001, 10])\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, group in pp_ren.groupby(pp_ren.pargp.apply(remove_numbers)):\n",
    "    pp_ren.loc[group.index,['parlbnd','parubnd']] = bounds[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_ren.to_csv('pilot_point_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_ren.to_csv(os.path.join(owhm2, 'pilot_point_info.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### geostats and kriging\n",
    "now that we have pilot points setup, we need to solve the kriging equations for each model cell using pilot point locations.  Since we only have a single set of pilot points that we are reusing for several array-based ``modflow`` inputs, we only need to get the kriging factors once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_pp = pyemu.pp_utils.pp_file_to_dataframe(os.path.join(ml.model_ws,'pp2024',\"hk1pp.dat\"))\n",
    "hk_pp2 = pyemu.pp_utils.pp_file_to_dataframe(os.path.join(ml.model_ws,'pp2024',\"hk2pp.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_pp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Let's setup a geostatistical structure.  The contribution doesn't matter for pilot point interpolation, but it does matter when we want to form a prior parameter covariance matrix - we will get to that later.  A good rule of thumb is to use an ``a`` value that is three times the pilot point spacing.  Also, since the all of these pilot points will be log transformed, we need to use a log-based geostatistical structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pp_cells * ml.dis.delr.array[0] * 3.0\n",
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=a)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,transform=\"log\")\n",
    "gs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "This is where things get fun.  First we create an ``OrdinaryKrige`` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = pyemu.geostats.OrdinaryKrige(geostruct=gs,point_data=hk_pp)\n",
    "ok2 = pyemu.geostats.OrdinaryKrige(geostruct=gs,point_data=hk_pp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Now we use a helper function to solve the kriging factors for each active model cell: ``OrdinaryKrige.calc_factors_grid()`` includes all the standard kriging arguments, such as search radius, min and max interpolation points,zone_array, as well as the option to save the kriging variance array \n",
    "\n",
    "Note: we need to pass out model's spatial reference information. For flopy this used to be contained in model.sr\n",
    "      However this has been superseded by model.modelgrid. To avoid reliance on a changing (and not always backward\n",
    "      compatible) code base the sr method has been abstracted into pyemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(zones[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = pyemu.helpers.SpatialReference.from_namfile(os.path.join(ml.model_ws, ml.namefile),\n",
    "#                                                  delr=ml.dis.delr, delc=ml.dis.delc)\n",
    "ok.calc_factors_grid(sr, \n",
    "                     # zone_array=zones[0],\n",
    "                     var_filename=os.path.join(ml.model_ws,'pp2024',\"layer1_var.dat\"))\n",
    "print('\\nstarting layer 2\\n')\n",
    "ok2.calc_factors_grid(sr, \n",
    "                     # zone_array=zones[1],\n",
    "                     var_filename=os.path.join(ml.model_ws,'pp2024',\"layer2_var.dat\"))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Ok, we know that this function is slow for bigly models, but it is super convienent and allows a lot of flexibility.  So, once we have calculated the kriging factors for each active model cell, we need to write this to a factors file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.to_grid_factors_file(os.path.join(ml.model_ws,'pp2024', \"pp.fac\"))\n",
    "ok2.to_grid_factors_file(os.path.join(ml.model_ws,'pp2024', \"pp2.fac\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# this has been added to helpers for model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(forward_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def write_pilot_point(layer, prop, model_ws):\n",
    "#     if layer !=1:\n",
    "#         factors_file = os.path.join(model_ws,'pp2024', \"pp.fac\")\n",
    "#     else:\n",
    "#         factors_file = os.path.join(model_ws,'pp2024', \"pp2.fac\")\n",
    "\n",
    "#     out_file = os.path.join(model_ws, 'pp2024_out',f\"{prop}.txt\")\n",
    "    \n",
    "#     pp_file =  os.path.join(model_ws, 'pp2024',f\"{prop}pp.dat\")\n",
    "#     assert os.path.exists(pp_file), f\"pp_file does not exist {pp_file}\"\n",
    "#     print(f\"pp_file = {pp_file}, factors_file={factors_file}, out_file={out_file}\")\n",
    "    \n",
    "#     hk_arr = pyemu.geostats.fac2real(pp_file, factors_file=factors_file, out_file=out_file)\n",
    "\n",
    "# def write_all_pp(model_ws):\n",
    "#     prefix_dict= {0:[\"hk1\",\"sy1\",\"vk1\"],\n",
    "#                  1:[\"hk2\",\"ss2\",\"vk2\",'fmp_vk', 'drn_k'],\n",
    "#                  2:[\"hk3\",\"ss3\",\"vk3\"],\n",
    "#                  3:[\"hk4\",\"ss4\",\"vk4\"],\n",
    "#                  4:[\"hk5\",\"ss5\",\"vk5\"],\n",
    "#                  5:[\"hk6\",\"ss6\",\"vk6\"]}\n",
    "#     for lay in prefix_dict.keys():\n",
    "#         for par in prefix_dict[lay]:\n",
    "#             write_pilot_point(lay, par, model_ws)\n",
    "\n",
    "forward_run.write_all_pp(ml.model_ws)\n",
    "\n",
    "\n",
    "# pp_file = 'hkpp1.dat'\n",
    "\n",
    "# out_folder = ''\n",
    "# out_file='freyberg6.npf_k_layer1.txt'\n",
    "\n",
    "# pp_file = 'rchpp.dat'\n",
    "# hk_arr = pyemu.geostats.fac2real(pp_file, factors_file=pp_file+'.fac',out_file='rch0_fac.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Let's check out that kriging variance array....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_arr = np.ma.masked_invalid(np.loadtxt(os.path.join(ml.model_ws,'pp2024',\"layer1_var.dat\")))\n",
    "fig = plt.figure(figsize=(8.5,11))\n",
    "mm = conda_scripts.make_map.make_map('layer1')\n",
    "ax = mm.plotloc(fig, locname = 'SON_MOD')\n",
    "ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,var_arr,alpha=0.5)\n",
    "ax.scatter(hk_pp.x, hk_pp.y,marker='o', c = hk_pp.zone,s=25, cmap = 'tab10', ec = 'k')\n",
    "plt.savefig('pilot_points_lay1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_pp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_arr = np.ma.masked_invalid(np.loadtxt(os.path.join(ml.model_ws,'pp2024',\"layer2_var.dat\")))\n",
    "fig = plt.figure(figsize=(8.5,11))\n",
    "mm = conda_scripts.make_map.make_map('layer2')\n",
    "ax = mm.plotloc(fig, locname = 'SON_MOD')\n",
    "ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,var_arr,alpha=0.5)\n",
    "ax.scatter(hk_pp2.x, hk_pp2.y,marker='o', c = hk_pp2.zone,s=25, cmap = 'tab10', ec = 'k')\n",
    "plt.savefig('pilot_points_lay2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# add zone multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_path = os.path.join('zone_pest_mult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(mult_path):\n",
    "    os.makedirs(mult_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "zdict = forward_run.get_prefix_dict_for_pilot_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.genfromtxt(os.path.join(ml.model_ws, 'model_arrays', 'zonation_3.csv'), delimiter = ' ')\n",
    "\n",
    "zotther = z.copy()\n",
    "zotther[zotther>8] = 0\n",
    "\n",
    "zones = {i:zotther  for i in range(7) }\n",
    "zones[0] = z\n",
    "plt.imshow(zones[1])\n",
    "\n",
    "z = conda_scripts.arich_functions.array2rc(zones[1],'zone').astype({'zone':int})\n",
    "\n",
    "aliases = {1: 'Bay', 2: 'EastSide', 3: 'SouthCent', 4: 'Kenwood', 5: 'VOM', 6: 'AguaCal',7:'WestSide',8:'CitySon',9:'Highlands'}\n",
    "z.loc[:,'zone'] =z.loc[:,'zone'].replace(aliases)\n",
    "z = z.query(\"zone!=0\")\n",
    "\n",
    "geoms = conda_scripts.arich_functions.get_model_shp(ml.modelgrid, 2226).drop(columns = ['row','col'])\n",
    "\n",
    "z = gpd.GeoDataFrame(pd.merge(z, geoms, on = ['i','j']))\n",
    "zplot = z.dissolve('zone').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = [z for _,z in zones.items()]\n",
    "zz = np.stack(zz)\n",
    "zz = zz[:6,:,:]\n",
    "zz[ml.bas6.ibound.array==0] = np.nan\n",
    "zz = np.array(zz, dtype = int)\n",
    "zz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(6):\n",
    "\n",
    "    ax[i].imshow(zz[i])\n",
    "    ax[i].set_title(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = forward_run.get_prefix_dict_for_pilot_points()\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# write array files while adding parameter locations for paremeter values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('zonemult_hk2_Highlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lay, vals in prefix.items():\n",
    "    for val in vals:\n",
    "        # string_array = np.zeros(z.shape, dtype ='U20')\n",
    "        string_array = np.zeros(zz[i].shape, dtype ='U30')\n",
    "        string_array[:,:] = '1e30'\n",
    "        for zonenum, zonename in aliases.items():\n",
    "            \n",
    "            c_ = zz[lay]==zonenum\n",
    "            fff = \"zonemult_{:}_{:}\".format(val, zonename)\n",
    "            string_array[c_] = f\"~{fff:20s}~\"\n",
    "            print(lay, zonenum, val, zonename, c_.ravel().sum())\n",
    "            # print(np.unique(base.ravel()))\n",
    "            # # Use np.vectorize to apply the dictionary mapping\n",
    "            # string_array = np.vectorize(lambda x: '' if x is None else aliases.get(x))(base)\n",
    "        file = os.path.join(mult_path, f'zonemult_{val}.csv.tpl')\n",
    "        \n",
    "        with open(file,'w') as wrt:\n",
    "            wrt.write('ptf ~\\n')\n",
    "        with open(file,'a') as wrt:\n",
    "            np.savetxt(wrt, string_array, fmt = '%s', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "# create layer multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lay, vals in prefix.items():\n",
    "    for val in vals:       \n",
    "        \n",
    "        string_array = np.zeros(zz[lay].shape, dtype ='U30')\n",
    "        string_array[:,:] = '1e30'\n",
    "\n",
    "        c_ = zz[lay]>0\n",
    "        \n",
    "        fff = \"laymult_{:}\".format(val)\n",
    "        string_array[c_] = f\"~{fff:20s}~\"\n",
    "\n",
    "        file = os.path.join(mult_path, f'laymult_{val}.csv.tpl')\n",
    "        \n",
    "        with open(file,'w') as wrt:\n",
    "            wrt.write('ptf ~\\n')\n",
    "        with open(file,'a') as wrt:\n",
    "            np.savetxt(wrt, string_array, fmt = '%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "# create zone multiplier only  (ie constant for all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric = re.sub(r'\\d', '', 'sy1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvals = []\n",
    "for lay, vals in prefix.items():\n",
    "    for val in vals:\n",
    "        # string_array = np.zeros(z.shape, dtype ='U20')\n",
    "        string_array = np.zeros(zz[0].shape, dtype ='U30')\n",
    "        string_array[:,:] = '1e30'\n",
    "        for zonenum, zonename in aliases.items():\n",
    "            \n",
    "            c_ = zz[lay]==zonenum\n",
    "            non_numeric = re.sub(r'\\d', '', val)\n",
    "            fff = \"zoneentire_{:}_{:}\".format(non_numeric, zonename.lower())\n",
    "            string_array[c_] = f\"~{fff:20s}~\"\n",
    "            # print(fff, c_.ravel().sum())\n",
    "            allvals.append(fff)\n",
    "            # print(np.unique(base.ravel()))\n",
    "            # # Use np.vectorize to apply the dictionary mapping\n",
    "            # string_array = np.vectorize(lambda x: '' if x is None else aliases.get(x))(base)\n",
    "        file = os.path.join(mult_path, f'zoneentire_{val}.csv.tpl')\n",
    "        print(file)\n",
    "        with open(file,'w') as wrt:\n",
    "            wrt.write('ptf ~\\n')\n",
    "        with open(file,'a') as wrt:\n",
    "            np.savetxt(wrt, string_array, fmt = '%s', delimiter=',')\n",
    "\n",
    "print('\\n\\nthese are the parameters in the above files\\n')\n",
    "print('\\n'.join(list(sorted(set(allvals)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
